# 零、进程的概念

#### 1.什么是进程

```
进程是正在进行的一个过程或者任务，负责执行任务的则是CPU
```

#### 2.进程与程序的区别

```
程序并不能单独运行，只有将程序装载到内存中，系统为它分配资源之后才能运行，而这种执行的程序称之为进程；
程序和进程的区别就在于：
	程序是指令的集合，它是进程运行的静态描述版本；进程是程序的一次执行活动，属于动态概念；
同一个程序执行两次是两个进程。
```

#### 3.并发与并行

```
并发：是伪并行，单个CPU下多个程序交替运行；单个CPU+多道技术就可以实现并发
并行：同时运行，多个CPU下多个程序同时运行；
```

#### 4.同步/异步和阻塞/非阻塞

```python
# 小故事
普通水壶，简称水壶；会响的水壶，简称响水壶，具体如下：

同步阻塞：小明把水壶放到火上，然后在那傻等水开
同步非阻塞：小明把水壶放到火上，然后去客厅看电视，时不时的去厨房看看水开没有
异步阻塞：小明把响水壶放到火上，然后在那傻等水开
异步非阻塞：小明把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去处理

# 细节
同步/异步：形容一次方法的调用，在单线程中。描述的是执行者是否具备主动通知功能
　　同步，调用者会等到方法调用返回后才能继续后面的行为
　　异步，调用者不需要等到方法返回，方法执行完毕后会主动通知调用者

阻塞/非阻塞：调用者是否可以执行多个任务，在多个线程中。描述的是调用者的多个线程是否可以同时执行
　　阻塞，线程1和线程2不能同时进行
　　非阻塞，线程1和线程2可以同时进行
```

### 一、multiprocessing模块介绍

```
python中的多线程无法利用多核优势，如果想要充分地使用多核CPU的资源，在python中大部分情况需要使用多进程。Python提供了multiprocessing。

multiprocessing模块用来开启子进程，并在子进程中执行我们定制的任务（比如函数），multiprocessing模块的功能众多：支持子进程、通信和共享数据、执行不同形式的同步，提供了Process、Queue、Pipe、Lock等组件。

与线程不同，进程没有任何共享状态，进程修改的数据，改动仅限于该进程内 
```

### 二、process类的使用

#### 1.创建进程的类

```python
Process([group [, target [, name [, args [, kwargs]]]]])，由该类实例化得到的对象，表示一个子进程中的任务（尚未启动）

#强调：
1. 需要使用关键字的方式来指定参数
2. args指定的为传给target函数的位置参数，是一个元组形式，必须有逗号

#参数介绍
1. group参数未使用，值始终为None
2. target表示调用对象，即子进程要执行的任务
3. args表示调用对象的位置参数元组，args=(1,2,'anne',)
4. kwargs表示调用对象的字典,kwargs={'name':'anne','age':18}
5. name为子进程的名称
```

#### 2.创建并开启进程的两种方式

###### 2.1方式一

```python
import time
import random
from multiprocessing import Process

def run(name):
    print('%s runing' % name)
    time.sleep(random.randrange(1, 5))
    print('%s running end' % name)

if __name__ == '__main__':
    p1 = Process(target=run, args=('anne',))  # 必须加','号
    p2 = Process(target=run, args=('alice',))
    p3 = Process(target=run, args=('biantai',))
    p4 = Process(target=run, args=('haha',))

    p1.start()
    p2.start()
    p3.start()
    p4.start()
    print('主线程')
```

###### 2.2方式二

```python
import time
import random
from multiprocessing import Process

class Run(Process):
    def __init__(self, name):
        super().__init__()
        self.name = name

    def run(self):
        print('%s runing' % self.name)
        time.sleep(random.randrange(1, 5))
        print('%s runing end' % self.name)

if __name__ == '__main__':
    p1 = Run('anne')
    p2 = Run('alex')
    p3 = Run('ab')
    p4 = Run('hey')
    p1.start()  # start会自动调用run
    p2.start()
    p3.start()
    p4.start()
    print('主线程')
```

#### 3.process对象的join方法

```python
import time
import random
from multiprocessing import Process

class Run(Process):
    def __init__(self, name):
        super().__init__()
        self.name = name

    def run(self):
        print('%s runing' % self.name)
        time.sleep(random.randrange(1, 5))
        print('%s runing end' % self.name)

if __name__ == '__main__':
    p1 = Run('anne')
    p2 = Run('alex')
    p3 = Run('ab')
    p4 = Run('hey')
    
    p_list = [p1,p2,p3,p4]
    for p in p_list:
        p.start()
        
    for p in p_list:
        p.join()
```

### 三、守护进程

```python
主进程创建守护进程
1）守护进程会在主进程代码执行结束后就终止
2）守护进程内无法再开启子进程,否则抛出异常：
	AssertionError: daemonic processes are not allowed to have children
注意：进程之间是互相独立的，主进程代码运行结束，守护进程随即终止
```

#### 1.示例一

```python
from multiprocessing import Process
import time
import random

class Run(Process):
    def __init__(self,name):
        self.name=name
        super().__init__()
        
    def run(self):
        print('%s is piaoing' %self.name)
        time.sleep(random.randrange(1,3))
        print('%s is piao end' %self.name)

if __name__ == '__main__':
    p = Run('anne')
    # 一定要在p.start()前设置,设置p为守护进程,禁止p创建子进程,并且父进程代码执行结束,p即终止运行
    p.daemon = True
    p.start()
    print('主')
    
#结果
	主
```

#### 2.示例二

```python
from multiprocessing import Process
import time

def foo():
    print(123)
    time.sleep(1)
    print("end123")

def bar():
    print(456)
    time.sleep(3)
    print("end456")

if __name__ == '__main__':
    p1 = Process(target=foo)
    p2 = Process(target=bar)

    p1.daemon = True
    p1.start()
    p2.start()
    print("main-------")  # 打印该行则主进程代码结束,则守护进程p1应该被终止,可能会有p1任务执行的打印信息123,因为主进程打印main----时,p1也执行了,但是随即被终止

#结果
	main-------
    456
    end456
```

### 四、进程同步（锁）

```
进程之间数据不共享,但是共享同一套文件系统,所以访问同一个文件,或同一个打印终端,是没有问题的,
而共享带来的是竞争，竞争带来的结果就是错乱，如何控制，就是加锁处理。
```

#### 1.示例一：多个进程共享一个打印终端

```python
#并发运行,效率高,但竞争同一打印终端,带来了打印错乱
from multiprocessing import Process
import os,time

def work():
    print('%s is running' %os.getpid())
    time.sleep(2)
    print('%s is done' %os.getpid())

if __name__ == '__main__':
    for i in range(3):
        p=Process(target=work)
        p.start()
        
#结果
7928 is running
5480 is running
18516 is running
7928 is done
5480 is done
18516 is done
```

```python
# 由并发变成了串行,牺牲了运行效率,但避免了竞争，实现了顺序打印
from multiprocessing import Process, Lock
import os, time

def work(lock):
    lock.acquire()
    print('%s is running' % os.getpid())
    time.sleep(2)
    print('%s is done' % os.getpid())
    lock.release()

if __name__ == '__main__':
    lock = Lock()
    for i in range(3):
        p = Process(target=work, args=(lock,))
        p.start()
        
#结果
18388 is running
18388 is done
18464 is running
18464 is done
19004 is running
19004 is done
```

#### 2.示例二：多个进程共享一个文件

```python
#文件db的内容为：{"count":1}
#注意一定要用双引号，不然json无法识别
#并发运行，效率高，但竞争写同一文件，数据写入错乱
from multiprocessing import Process, Lock
import time, json, random

def search():
    dic = json.load(open('db.txt'))
    print('剩余票数%s' % dic['count'])

def get():
    dic = json.load(open('db.txt'))
    time.sleep(0.1)  # 模拟读数据的网络延迟
    if dic['count'] > 0:
        dic['count'] -= 1
        time.sleep(0.2)  # 模拟写数据的网络延迟
        json.dump(dic, open('db.txt', 'w'))
        print('购票成功')

def task(lock):  #不加锁，会导致抢票混乱
    search()
    get()

if __name__ == '__main__':
    lock = Lock()
    for i in range(100):  # 模拟并发100个客户端抢票
        p = Process(target=task, args=(lock,))
        p.start()
        
#结果
	抢票结果是乱的，比如票有5张，会出现多于5个人抢到票
```

```python
#文件db的内容为：{"count":1}
#注意一定要用双引号，不然json无法识别
#并发运行，效率高，但竞争写同一文件，数据写入错乱
from multiprocessing import Process, Lock
import time, json, random

def search():
    dic = json.load(open('db.txt'))
    print('剩余票数%s' % dic['count'])

def get():
    dic = json.load(open('db.txt'))
    time.sleep(0.1)  # 模拟读数据的网络延迟
    if dic['count'] > 0:
        dic['count'] -= 1
        time.sleep(0.2)  # 模拟写数据的网络延迟
        json.dump(dic, open('db.txt', 'w'))
        print('购票成功')

def task(lock):  #不加锁，会导致抢票混乱
    search()
    lock.acquire()   #获取锁
    get()
    lock.release()   #释放锁

if __name__ == '__main__':
    lock = Lock()
    for i in range(100):  # 模拟并发100个客户端抢票
        p = Process(target=task, args=(lock,))
        p.start()
        
#结果
	加锁后，抢票就正常了，有几张票，就会有几个人会抢到票
```

```
加锁可以保证多个进程修改同一块数据时，同一时间只能有一个任务可以进行修改，即串行的修改，没错，速度是慢了，但牺牲了速度却保证了数据安全。
```

### 五、进程间通信

```python
虽然可以用文件共享数据实现进程间通信，但问题是：
1.效率低（共享数据基于文件，而文件是硬盘上的数据） 
2.需要自己加锁处理

因此我们最好找寻一种解决方案能够兼顾：
1.效率高（多个进程共享一块内存的数据）
2.帮我们处理好锁问题。

mutiprocessing模块为我们提供的基于消息的IPC通信机制：队列和管道。
1.队列和管道都是将数据存放于内存中
2.队列又是基于（管道+锁）实现的，可以让我们从复杂的锁问题中解脱出来，我们应该尽量避免使用共享数据，尽可能使用消息传递和队列，避免处理复杂的同步和锁问题，而且在进程数目增多时，往往可以获得更好的可获展性
```

#### 1.队列

```python
Queue([maxsize]):创建共享的进程队列，Queue是多进程安全的队列，可以使用Queue实现多进程之间的数据传递。 
#参数介绍
	maxsize是队列中允许最大项数，省略则无大小限制。  
```

```python
'''
multiprocessing模块支持进程间通信的两种主要形式:管道和队列,都是基于消息传递实现的,但是队列接口
'''
from multiprocessing import Process,Queue

q=Queue(3)

#put ,get ,put_nowait,get_nowait,full,empty
q.put(3)
q.put(3)
q.put(3)
print(q.full()) #满了

print(q.get())
print(q.get())
print(q.get())
print(q.empty()) #空了
```

#### 2.生产者消费者模型

```
生产者消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。
```

```python
from multiprocessing import Process,Queue
import time,random,os

def consumer(q):
    while True:
        res=q.get()
        if res is None:break #收到结束信号则结束
        time.sleep(random.randint(1,3))
        print('%s 吃 %s' %(os.getpid(),res))

def producer(q):
    for i in range(10):
        time.sleep(random.randint(1,3))
        res='包子%s' %i
        q.put(res)
        print('%s 生产了 %s' %(os.getpid(),res))
    q.put(None) #发送结束信号
    
if __name__ == '__main__':
    q=Queue()
    #生产者们:即厨师们
    p1=Process(target=producer,args=(q,))

    #消费者们:即吃货们
    c1=Process(target=consumer,args=(q,))

    #开始
    p1.start()
    c1.start()
    print('主')
```

```
注意：结束信号None，不一定要由生产者发，主进程里同样可以发，但主进程需要等生产者结束后才应该发送该信号
```

```python
from multiprocessing import Process,Queue
import time,random,os

def consumer(q):
    while True:
        res=q.get()
        if res is None:break #收到结束信号则结束
        time.sleep(random.randint(1,3))
        print('%s 吃 %s' %(os.getpid(),res))

def producer(q):
    for i in range(2):
        time.sleep(random.randint(1,3))
        res='包子%s' %i
        q.put(res)
        print('%s 生产了 %s' %(os.getpid(),res))

if __name__ == '__main__':
    q=Queue()
    #生产者们:即厨师们
    p1=Process(target=producer,args=(q,))

    #消费者们:即吃货们
    c1=Process(target=consumer,args=(q,))

    #开始
    p1.start()
    c1.start()

    p1.join()
    q.put(None) #发送结束信号
    print('主')

#主进程在生产者生产完毕后发送结束信号None

#结果
19296 生产了 包子0
11204 吃 包子0
19296 生产了 包子1
主
11204 吃 包子1
```

```
但上述解决方式，在有多个生产者和多个消费者时，应该怎么做呢？有几个消费者就发几次信号？
```

```python
from multiprocessing import Process,Queue
import time,random,os

def consumer(q):
    while True:
        res=q.get()
        if res is None:break #收到结束信号则结束
        time.sleep(random.randint(1,3))
        print('%s 吃 %s' %(os.getpid(),res))

def producer(name,q):
    for i in range(2):
        time.sleep(random.randint(1,3))
        res='%s%s' %(name,i)
        q.put(res)
        print('%s 生产了 %s' %(os.getpid(),res))



if __name__ == '__main__':
    q=Queue()
    #生产者们:即厨师们
    p1=Process(target=producer,args=('包子',q))
    p2=Process(target=producer,args=('骨头',q))
    p3=Process(target=producer,args=('泔水',q))

    #消费者们:即吃货们
    c1=Process(target=consumer,args=(q,))
    c2=Process(target=consumer,args=(q,))

    #开始
    p1.start()
    p2.start()
    p3.start()
    c1.start()
    c2.start()

    p1.join() #必须保证生产者全部生产完毕,才应该发送结束信号
    p2.join()
    p3.join()
    q.put(None) #有几个消费者就应该发送几次结束信号None
    q.put(None) #发送结束信号
    print('主')
```

#### 3.JoinableQueue

```python
JoinableQueue([maxsize])
这就像是一个Queue对象，但队列允许项目的使用者通知生成者项目已经被成功处理。通知进程是使用共享的信号和条件变量来实现的。
 
#参数介绍：
    maxsize是队列中允许最大项数，省略则无大小限制。   
#方法介绍：
    JoinableQueue的实例p除了与Queue对象相同的方法之外还具有：
    q.task_done()：使用者使用此方法发出信号，表示q.get()的返回项目已经被处理。如果调用此方法的次数大于从队列中删除项目的数量，将引发ValueError异常
    q.join():生产者调用此方法进行阻塞，直到队列中所有的项目均被处理。阻塞将持续到队列中的每个项目均调用q.task_done（）方法为止
```

```python
from multiprocessing import Process,JoinableQueue
import time,random,os

def consumer(q):
    while True:
        res=q.get()
        time.sleep(random.randint(1,3))
        print('\033[45m%s 吃 %s\033[0m' %(os.getpid(),res))

        q.task_done() #向q.join()发送一次信号,证明一个数据已经被取走了

def producer(name,q):
    for i in range(10):
        time.sleep(random.randint(1,3))
        res='%s%s' %(name,i)
        q.put(res)
        print('\033[44m%s 生产了 %s\033[0m' %(os.getpid(),res))
    q.join()

if __name__ == '__main__':
    q=JoinableQueue()
    #生产者们:即厨师们
    p1=Process(target=producer,args=('包子',q))
    p2=Process(target=producer,args=('骨头',q))
    p3=Process(target=producer,args=('泔水',q))

    #消费者们:即吃货们
    c1=Process(target=consumer,args=(q,))
    c2=Process(target=consumer,args=(q,))
    c1.daemon=True
    c2.daemon=True

    #开始
    p_l=[p1,p2,p3,c1,c2]
    for p in p_l:
        p.start()

    p1.join()
    p2.join()
    p3.join()
    print('主') 
    
    #主进程等--->p1,p2,p3等---->c1,c2
    #p1,p2,p3结束了,证明c1,c2肯定全都收完了p1,p2,p3发到队列的数据
    #因而c1,c2也没有存在的价值了,应该随着主进程的结束而结束,所以设置成守护进程
```

#### 3.管道

#### 4.共享数据

#### 5.信号量

```python
互斥锁 同时只允许一个线程更改数据，而Semaphore是同时允许一定数量的线程更改数据 ，比如厕所有3个坑，那最多只允许3个人上厕所，后面的人只能等里面有人出来了才能再进去，如果指定信号量为3，那么来一个人获得一把锁，计数加1，当计数等于3时，后面的人均需要等待。一旦释放，就有人可以获得一把锁。

信号量与进程池的概念很像，但是要区分开，信号量涉及到加锁的概念

from multiprocessing import Process,Semaphore
import time,random

def go_wc(sem,user):
    sem.acquire()
    print('%s 占到一个茅坑' %user)
    time.sleep(random.randint(0,3)) #模拟每个人拉屎速度不一样，0代表有的人蹲下就起来了
    sem.release()

if __name__ == '__main__':
    sem=Semaphore(5)
    p_l=[]
    for i in range(13):
        p=Process(target=go_wc,args=(sem,'user%s' %i,))
        p.start()
        p_l.append(p)

    for i in p_l:
        i.join()
    print('============》')
```

### 六、进程池

```
利用Python进行系统管理的时候，特别是同时操作多个文件目录，或者远程控制多台主机，并行操作可节约大量的时间。多进程是实现并发的手段之一，需要注意的问题是：
1.很明显需要并发执行的任务通常要远大于核数
2.一个操作系统不可能无限开启进程，通常有几个核就开几个进程
3.进程开启过多，效率反而会下降（开启进程是需要占用系统资源的，而且开启多余核数目的进程也无法做到并行）

例如当被操作对象数目不大时，可以直接利用multiprocessing中的Process动态成生多个进程，十几个还好，但如果是上百个，上千个。。。手动的去限制进程数量却又太过繁琐，此时可以发挥进程池的功效。

我们就可以通过维护一个进程池来控制进程数目，比如httpd的进程模式，规定最小进程数和最大进程数... 

对于远程过程调用的高级应用程序而言，应该使用进程池，Pool可以提供指定数量的进程，供用户调用，当有新的请求提交到pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，就重用进程池中的进程。
```

#### 1.创建进程池的类

```python
Pool([numprocess  [,initializer [, initargs]]])
#参数介绍
1.numprocess:要创建的进程数，如果省略，将默认使用cpu_count()的值
2.initializer：是每个工作进程启动时要执行的可调用对象，默认为None
3.initargs：是要传给initializer的参数组

#主要方法
1. p.apply(func [, args [, kwargs]])
	在一个池工作进程中执行func(*args,**kwargs),然后返回结果。
	需要强调的是：此操作并不会在所有池工作进程中并执行func函数。如果要通过不同参数并发地执行func函数，必须从不同线程调用p.apply()函数或者使用p.apply_async()
    
2. p.apply_async(func [, args [, kwargs]]):
	在一个池工作进程中执行func(*args,**kwargs),然后返回结果。
	此方法的结果是AsyncResult类的实例，callback是可调用对象，接收输入参数。当func的结果变为可用时，
将理解传递给callback。callback禁止执行任何阻塞操作，否则将接收其他异步操作中的结果。

3. p.close():关闭进程池，防止进一步操作。如果所有操作持续挂起，它们将在工作进程终止前完成
4. P.join():等待所有工作进程退出。此方法只能在close()或teminate()之后调用
```

#### 2.应用

```python
#一：使用进程池（异步调用,apply_async）
from multiprocessing import Pool
import time

def func(msg):
    print( "msg:", msg)
    time.sleep(1)
    return msg

if __name__ == "__main__":
    p = Pool(processes = 3)
    res_l=[]
    for i in range(10):
        msg = "hello %d" %(i)
        res=p.apply_async(func, (msg, ))   #维持执行的进程总数为processes，当一个进程执行完毕后会添加新的进程进去
        res_l.append(res)
    print("==============================>") #没有后面的join，或get，则程序整体结束，进程池中的任务还没来得及全部执行完也都跟着主进程一起结束了

    p.close()  #关闭进程池，防止进一步操作。如果所有操作持续挂起，它们将在工作进程终止前完成
    p.join()   #调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束

    print(res_l) #看到的是<multiprocessing.pool.ApplyResult object at 0x10357c4e0>对象组成的列表,而非最终的结果,但这一步是在join后执行的,证明结果已经计算完毕,剩下的事情就是调用每个对象下的get方法去获取结果
    for i in res_l:
        print(i.get()) #使用get来获取apply_aync的结果,如果是apply,则没有get方法,因为apply是同步执行,立刻获取结果,也根本无需get

        
#二：使用进程池（同步调用,apply）
from multiprocessing import Pool
import time

def func(msg):
    print( "msg:", msg)
    time.sleep(0.1)
    return msg

if __name__ == "__main__":
    p = Pool(processes = 3)
    res_l=[]
    for i in range(10):
        msg = "hello %d" %(i)
        res=p.apply(func, (msg, ))#维持执行的进程总数为processes，一个进程执行完毕后会添加新进程
        res_l.append(res) #同步执行，即执行完一个拿到结果，再去执行另外一个
    print("==============================>")
    p.close()
    p.join()   #调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到p,join函数等待所有子进程结束

    print(res_l) #看到的就是最终的结果组成的列表
    for i in res_l: #apply是同步的，所以直接得到结果，没有get()方法
        print(i)
        
#三：使用进程池（同步调用,map）
from multiprocessing import Pool
import time

def func(msg):
    print('msg: ', msg)
    time.sleep(1)
    print('********')
    return 'func_return: %s' % msg

if __name__ == '__main__':
    args = [1, 2, 3, 4, 5, 6, ]
    p = Pool(processes=5)
    return_data = p.map(func, args)
    print('堵塞')  # 执行完func才执行该句
    p.close()
    p.join()  # join语句要放在close之后
    print(return_data)
    
#四：使用进程池（异步调用,map_async）
from multiprocessing import Pool
import time

def func(msg):
    print('msg: ', msg)
    time.sleep(1)
    print('********')
    return 'func_return: %s' % msg

if __name__ == '__main__':
    args = [1, 2, 4, 5, 7, 8]
    p = Pool(processes=5)
    result = p.map_async(func, args)
    print('不堵塞')
    p.close()
    p.join()
    for i in result.get():
        print(i)  # 进程函数返回值
```

###### 2.1回调函数

```
进程池中任何一个任务一旦处理完了，就立即告知主进程：我好了额，你可以处理我的结果了。主进程则调用一个函数去处理该结果，该函数即回调函数；

我们可以把耗时间（阻塞）的任务放到进程池中，然后指定回调函数（主进程负责执行），这样主进程在执行回调函数时就省去了I/O的过程，直接拿到的是任务的结果。
```

```python
from multiprocessing import Pool
import requests
import json
import os

def get_page(url):
    print('<进程%s> get %s' %(os.getpid(),url))
    respone=requests.get(url)
    if respone.status_code == 200:
        return {'url':url,'text':respone.text}

def parse_page(res):
    print('<进程%s> parse %s' %(os.getpid(),res['url']))
    parse_res='url:<%s> size:[%s]\n' %(res['url'],len(res['text']))
    with open('db.txt','a') as f:
        f.write(parse_res)


if __name__ == '__main__':
    urls=[
        'https://www.baidu.com',
        'https://www.python.org',
        'https://www.openstack.org',
        'https://help.github.com/',
        'http://www.sina.com.cn/'
    ]

    p=Pool(3)
    res_l=[]
    for url in urls:
        res=p.apply_async(get_page,args=(url,),callback=parse_page)
        res_l.append(res)

    p.close()
    p.join()
    print([res.get() for res in res_l]) #拿到的是get_page的结果,其实完全没必要拿该结果,该结果已经传给回调函数处理了

'''
打印结果:
<进程3388> get https://www.baidu.com
<进程3389> get https://www.python.org
<进程3390> get https://www.openstack.org
<进程3388> get https://help.github.com/
<进程3387> parse https://www.baidu.com
<进程3389> get http://www.sina.com.cn/
<进程3387> parse https://www.python.org
<进程3387> parse https://help.github.com/
<进程3387> parse http://www.sina.com.cn/
<进程3387> parse https://www.openstack.org
[{'url': 'https://www.baidu.com'
```

### ============================

# 零、线程

#### 1.什么是线程

```
线程是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。
一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同任务。
```

#### 2.进程与线程的区别

```python
1.根本区别
	进程是资源分配最小单位，线程是程序执行的最小单位
2.地址空间
	进程有独立的地址空间，每启动一个进程，系统都会为其分配地址空间，创建数据表维护代码段、堆栈段和数据段；
	线程没有独立的地址空间，同一进程的线程共享本进程的地址空间；
3.资源拥有
	进程之间的资源是独立的，同一进程内的线程共享本进程的资源
4.执行过程
	每个独立的进程有一个程序运行的入口、顺序执行序列
	线程不能单独执行，必须依赖应用程序中，由应用程序提供多个线程执行控制
5.系统开销
	进程执行开销大，线程执行开销小
```

### 一、threading模块介绍

#### 1.开启线程的两种方式

```python
# 直接调用
from threading import Thread
import time
def run(n):
    print('task',n)
    time.sleep(2)

t1 = Thread(target=run,args=('t1',))
t1.start()
```

```python
#继承式调用
from threading import Thread
import time
class MyThread(Thread):
    def __init__(self,n,sleep_time):
        super().__init__()
        self.n = n
        self.sleep_time = sleep_time

    def run(self):
        print('running task',self.n)
        time.sleep(self.sleep_time)
        print('task done,',self.n)

t1 = MyThread('t1',2)
t1.start()
```

#### 2.进程下开多线程与进程下开多子进程的区别

```python
from threading import Thread
from multiprocessing import Process
import os

def work():
    print('hello')

if __name__ == '__main__':
    #在主进程下开启线程
    t=Thread(target=work)
    t.start()
    print('主线程/主进程')
    '''
    打印结果:
    hello
    主线程/主进程
    '''

    #在主进程下开启子进程
    t=Process(target=work)
    t.start()
    print('主线程/主进程')
    '''
    打印结果:
    主线程/主进程
    hello
    '''
    
说明：从打印结果来看，开启线程的速度比开启进程的速度要快
```

```python
from threading import Thread
from multiprocessing import Process
import os

def work():
    print('hello',os.getpid())

if __name__ == '__main__':
    # part1:在主进程下开启多个线程,每个线程都跟主进程的pid一样
    t1=Thread(target=work)
    t2=Thread(target=work)
    t1.start()
    t2.start()
    print('主线程/主进程pid',os.getpid())

    # part2:在主进程下开多个子进程,每个进程都有不同的pid
    p1=Process(target=work)
    p2=Process(target=work)
    p1.start()
    p2.start()
    print('主线程/主进程pid',os.getpid())
```

```python
from  threading import Thread
from multiprocessing import Process
import os
def work():
    global n
    n=0

if __name__ == '__main__':
    # n=100
    # p=Process(target=work)
    # p.start()
    # p.join()
    # print('主',n) #毫无疑问子进程p已经将自己的全局的n改成了0,但改的仅仅是它自己的,查看父进程的n仍然为100


    n=1
    t=Thread(target=work)
    t.start()
    t.join()
    print('主',n) #查看结果为0,因为同一进程内的线程之间共享进程内的数据
```

#### 3.join方法

```python
from threading import Thread
import time

def sayhi(name):
    time.sleep(2)
    print('%s say hello' %name)

if __name__ == '__main__':
    t=Thread(target=sayhi,args=('egon',))
    t.start()
    t.join()    #主线程等待子线程运行结束了再往下走
    print('主线程')
    print(t.is_alive())
    '''
    egon say hello
    主线程
    False
    '''
```

### 二、守护线程

```python
无论是进程还是线程，都遵循：守护(进程/线程)会等待主(进程/线程)运行完毕后被销毁

1.对主进程来说，运行完毕指的是主进程代码运行完毕
2.对主线程来说，运行完毕指的是主线程所在的进程内所有非守护线程统统运行完毕，主线程才算运行完毕

需要强调的是：运行完毕并非终止运行

# 详细解释
1.主进程在其代码结束后就已经算运行完毕了（守护进程在此时就被回收）,然后主进程会一直等非守护的子进程都运行完毕后回收子进程的资源(否则会产生僵尸进程)，才会结束。

2.主线程在其他非守护线程运行完毕后才算运行完毕（守护线程在此时就被回收）。因为主线程的结束意味着进程的结束，进程整体的资源都将被回收，而进程必须保证非守护线程都运行完毕后才能结束。
```

```python
from threading import Thread
import time
def sayhi(name):
    time.sleep(2)
    print('%s say hello' %name)

if __name__ == '__main__':
    t=Thread(target=sayhi,args=('egon',))
    t.setDaemon(True) #必须在t.start()之前设置
    t.start()

    print('主线程')
    print(t.is_alive()) #结果为True说明此时主线程并没结束，守护线程还在
    '''
    主线程
    True
    '''
```

```python
from threading import Thread
import time
def foo():
    print(123)
    time.sleep(3)
    print("end123")

def bar():
    print(456)
    time.sleep(1)
    print("end456")


t1=Thread(target=foo)
t2=Thread(target=bar)

t1.daemon=True  #将t1设置为守护线程，主线程结束后t1也结束，
t1.start()  #可能会出现t1没有完全走完就结束的情况
t2.start()
print("main-------")

"""
运行结果：
123
456
main-------
end456
"""
```

### 三、锁

#### 1.锁的使用

```python
锁通常被用来实现对共享资源的同步访问。为每一个共享资源创建一个Lock对象，当你需要访问该资源时，调用acquire方法来获取锁对象（如果其它线程已经获得了该锁，则当前线程需等待其被释放），待资源访问完后，再调用release方法释放锁

from threading import Lock

R = Lock()

R.acquire()  #获取所对象
'''
对公共数据的操作
'''
R.release()  #释放
```

#### 2.GIL锁与互斥锁分析

```python
1.100个线程去抢GIL锁，即抢执行权限
2.肯定有一个线程先抢到GIL（暂且称为线程1），然后开始执行，一旦执行就会拿到lock.acquire()
3.极有可能线程1还未运行完毕，就有另外一个线程2抢到GIL，然后开始运行，但线程2发现互斥锁lock还未被线程1释放，于是阻塞，被迫交出执行权限，即释放GIL
4.直到线程1重新抢到GIL，开始从上次暂停的位置继续执行，直到正常释放互斥锁lock，然后其他的线程再重复2 3 4的过程
```

#### 3.互斥锁与join的区别

```python
#不加锁:并发执行,速度快,数据不安全
from threading import current_thread,Thread,Lock
import os,time
def task():
    global n
    print('%s is running' %current_thread().getName())
    temp=n
    time.sleep(0.5)
    n=temp-1


if __name__ == '__main__':
    n=100
    lock=Lock()
    threads=[]
    start_time=time.time()
    for i in range(100):
        t=Thread(target=task)
        threads.append(t)
        t.start()
    for t in threads:
        t.join()

    stop_time=time.time()
    print('主:%s n:%s' %(stop_time-start_time,n))

'''
Thread-1 is running
Thread-2 is running
......
Thread-100 is running
主:0.5216062068939209 n:99
'''
```

```python
#加锁:未加锁部分并发执行,加锁部分串行执行,速度慢,数据安全
from threading import current_thread,Thread,Lock
import os,time
def task():
    #未加锁的代码并发运行
    time.sleep(3)
    print('%s start to run' %current_thread().getName())
    global n
    #加锁的代码串行运行
    lock.acquire()
    temp=n
    time.sleep(0.5)
    n=temp-1
    lock.release()

if __name__ == '__main__':
    n=100
    lock=Lock()
    threads=[]
    start_time=time.time()
    for i in range(100):
        t=Thread(target=task)
        threads.append(t)
        t.start()
    for t in threads:
        t.join()
    stop_time=time.time()
    print('主:%s n:%s' %(stop_time-start_time,n))

'''
Thread-1 is running
Thread-2 is running
......
Thread-100 is running
主:53.294203758239746 n:0
'''
```

```python
#思考:既然加锁会让运行变成串行,那么我在start之后立即使用join,就不用加锁了啊,也是串行的效果啊
#没错:在start之后立刻使用jion,肯定会将100个任务的执行变成串行,毫无疑问,最终n的结果也肯定是0,是安全的,但问题是
#start后立即join:任务内的所有代码都是串行执行的,而加锁,只是加锁的部分即修改共享数据的部分是串行的
#单从保证数据安全方面,二者都可以实现,但很明显是加锁的效率更高.
from threading import current_thread,Thread,Lock
import os,time
def task():
    time.sleep(3)
    print('%s start to run' %current_thread().getName())
    global n
    temp=n
    time.sleep(0.5)
    n=temp-1


if __name__ == '__main__':
    n=100
    lock=Lock()
    start_time=time.time()
    for i in range(100):
        t=Thread(target=task)
        t.start()
        t.join()
    stop_time=time.time()
    print('主:%s n:%s' %(stop_time-start_time,n))

'''
Thread-1 start to run
Thread-2 start to run
......
Thread-100 start to run
主:350.6937336921692 n:0 #耗时是多么的恐怖
'''
```

### 四、死锁现象与递归锁

```python
所谓死锁：是指两个或两个以上的进程或线程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程，如下就是死锁：

from threading import Thread,Lock
import time
mutexA=Lock()
mutexB=Lock()

class MyThread(Thread):
    def run(self):
        self.func1()
        self.func2()

    def func1(self):
        mutexA.acquire()
        print('\033[41m%s 拿到A锁\033[0m' %self.name)

        mutexB.acquire()
        print('\033[42m%s 拿到B锁\033[0m' %self.name)
        mutexB.release()
        mutexA.release()

    def func2(self):
        mutexB.acquire()
        print('\033[43m%s 拿到B锁\033[0m' %self.name)
        time.sleep(2)

        mutexA.acquire()
        print('\033[44m%s 拿到A锁\033[0m' %self.name)
        mutexA.release()

        mutexB.release()

if __name__ == '__main__':
    for i in range(10):
        t=MyThread()
        t.start()

'''
Thread-1 拿到A锁
Thread-1 拿到B锁
Thread-1 拿到B锁
Thread-2 拿到A锁
然后就卡住，死锁了
'''

解决方法，递归锁，在Python中为了支持在同一线程中多次请求同一资源，python提供了可重入锁RLock。

这个RLock内部维护着一个Lock和一个counter变量，counter记录了acquire的次数，从而使得资源可以被多次acquire。直到一个线程所有的acquire都被release，其他的线程才能获得资源。上面的例子如果使用RLock代替Lock，则不会发生死锁：
	mutexA=mutexB=threading.RLock() 
    #一个线程拿到锁，counter加1,该线程内又碰到加锁的情况，则counter继续加1，这期间所有其他线程都只能等待，等待该线程释放所有锁，即counter递减到0为止
```

### 五、信号量

### 六、线程queue

#### 1.queue.Queue(maxsize=0) #先进先出

```python
import queue

q=queue.Queue()
q.put('first')
q.put('second')
q.put('third')

print(q.get())
print(q.get())
print(q.get())
'''
结果(先进先出):
first
second
third
'''
```

#### 2.queue.LifoQueue(maxsize=0) #后进先出

```python
import queue

q=queue.LifoQueue()
q.put('first')
q.put('second')
q.put('third')

print(q.get())
print(q.get())
print(q.get())
'''
结果(后进先出):
third
second
first
'''
```

#### 3.queue.PriorityQueue(maxsize=0) #优先级队列

```python
import queue

q=queue.PriorityQueue()
#put进入一个元组,元组的第一个元素是优先级(通常是数字,也可以是非数字之间的比较),数字越小优先级越高
q.put((20,'a'))
q.put((10,'b'))
q.put((30,'c'))

print(q.get())
print(q.get())
print(q.get())
'''
结果(数字越小优先级越高,优先级高的优先出队):
(10, 'b')
(20, 'a')
(30, 'c')
'''
```

### 七、线程池

```python
from multiprocessing.dummy import Pool as ThreadPool
import time
 
def fun(msg):
    print('msg: ', msg)
    time.sleep(1)
    print('********')
    return 'fun_return %s' % msg
 
# map_async
arg = [1, 2, 10, 11, 18]
async_pool = ThreadPool(processes=4)
result = async_pool.map_async(fun, arg)
result.wait()  # 等待所有线程函数执行完毕
if result.ready():  # 线程函数是否已经启动了
    if result.successful():  # 线程函数是否执行成功
        print(result.get())  # 线程函数返回值
 
# map
arg = [3, 5, 11, 19, 12]
pool = ThreadPool(processes=3)
return_list = pool.map(fun, arg)
pool.close()
pool.join()
print(return_list)
 
# apply_async
async_pool = ThreadPool(processes=4)
results =[]
for i in range(5):
    msg = 'msg: %d' % i
    result = async_pool.apply_async(fun, (msg, ))
    results.append(result)
# async_pool.close()
# async_pool.join()
for i in results:
    i.wait()  # 等待线程函数执行完毕
 
for i in results:
    if i.ready():  # 线程函数是否已经启动了
        if i.successful():  # 线程函数是否执行成功
            print(i.get())  # 线程函数返回值
 
# apply
pool = ThreadPool(processes=4)
results =[]
for i in range(5):
    msg = 'msg: %d' % i
    result = pool.apply(fun, (msg, ))
    results.append(result)
 
print('apply: 堵塞')
print(results)
```

### 八、进程和线程的比喻例子

```
多线程和多进程的理解可以类比于公路。

假设当前公路均为单行道，并且出于安全考虑，一个车道只能同时行驶一辆汽车，一条公路只有一名驾驶员。只有一名指挥者进行集中调度，驾驶员获取到了指挥者的调度信息才会驾驶。

单线程是只有一条公路而且是单车道，只能同时行驶一辆汽车；

多线程是只有一条公路，但是是多车道，可以同时行驶多辆汽车；

多进程是有很多条公路，每条公路可能是单车道也可能是多车道，同样可以同时行驶多辆汽车。

因为GIL的存在，python中的多线程其实在同一时间只能运行一个线程，就像一名驾驶员只能同时驾驶一辆汽车。四线程类比于一条四车道的公路，但是驾驶员可以从驾驶车道A上的汽车切换至驾驶车道B上的汽车，驾驶员切换的速度够快的话，看起来就像是这条公路上的四辆汽车都在同时行驶。指挥者发布的命令只需要跨越车道就能传递给驾驶员，命令传输的时间损耗相对较小。所以对于多线程，我们希望指挥者可以比较频繁发布命令，驾驶员获取到命令后能够很快就完成然后切换到下一个车道继续执行命令，这样看起来就像是驾驶员同时驾驶四辆汽车。所以对于IO密集型代码，推荐使用多线程。

而对于多进程来说，每条公路都有一名驾驶员，四线程类比于四条公路，则四名驾驶员可以同时驾驶四辆汽车。但指挥者发布的命令需要跨越公路才能传递给驾驶员，命令传输的时间损耗相对较大。所以对于多进程，我们希望指挥者发布一次命令后驾驶员可以执行较长时间，这样就不必把时间过多花费在信息传输上。所以对于CPU密集型代码，推荐使用多进程。
```

### =========================

# 零、协程

#### 1.协程介绍

```python
协程：是单线程下的并发，又称微线程；
协程是一种用户态的轻量级线程，是由用户程序自己调度的；

对于单线程下，我们不可避免程序中出现io操作，但如果我们能在自己的程序中（即用户程序级别，而非操作系统级别）控制单线程下的多个任务能在一个任务遇到io阻塞时就切换到另外一个任务去计算，这样就保证了该线程能够最大限度的处于就绪状态（随时都可被CPU执行的状态），相当于我们在用户程序级别将自己的io操作最大限度的隐藏起来，从而可以迷惑操作系统让其看到：该线程好像是一直在计算，io比较少，从而更多的将CPU的执行权限分配给我们的线程；

#优点
1.协程的切换开销更小，属于程序级别的切换，操作系统完全感知不到，因而更加轻量级
2.单线程内就可以实现并发效果，最大限度的利用CPU

#缺点
1.协程的本质是单线程下，无法利用多核
2.协程指的是单个线程，一旦协程出现阻塞，将阻塞整个线程
```

### 一、asyncio+aiohttp

```python
from bs4 import BeautifulSoup
import time
import aiohttp
import asyncio

async def do_task(domain, pageUrl):
    async with aiohttp.ClientSession() as session:
        async with session.request('GET', pageUrl) as resp:
            html = await resp.read()  # 可直接获取bytes
            
    soup = BeautifulSoup(html, 'html.parser')
    for h in soup.select('h3>a'):
        url = ''.join([domain, h.get('href')])
        async with aiohttp.ClientSession() as session:
            async with session.request('GET', url) as resp:
                html = await resp.read()  # 可直接获取bytes
        print('url:{} title:{}'.format(url, parse_text(html)))

def parse_text(html):
    soup = BeautifulSoup(html, 'html.parser')
    return str(soup.select('.shici-title')[0].get_text())

def main():
    domain = 'http://www.shicimingju.com'
    urlTemplate = domain + '/chaxun/zuozhe/9_{0}.html'
    pageNum = 50  # 读取50页诗词进行测试
    loop = asyncio.get_event_loop()  # 获取事件循环
    tasks = []
    for num in range(pageNum):
        tasks.append(do_task(domain, urlTemplate.format(num + 1)))
    loop.run_until_complete(asyncio.wait(tasks))  # 协程
    loop.close()

if __name__ == '__main__':
    start = time.time()
    main()  # 调用方
    print('总耗时：%.5f秒' % float(time.time() - start))
```

### 二、asyncio模块

```

```











